{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811652d7-8b3b-4408-be7c-16ace1f553c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pymongo import DESCENDING\n",
    "from utils.fetchFileDetails import file_details\n",
    "from modules.modelBuilding import get_user_modelling_ads\n",
    "from utils.db import (\n",
    "    model_run_result,\n",
    "    modelling_ads_result_collection,\n",
    "    final_model_res_collection,\n",
    "\n",
    "    model_run_result,\n",
    "    modelling_ads_result_collection,\n",
    ")\n",
    "\n",
    "simulationOutput = Blueprint(\"simulationOutput\", __name__)\n",
    "\n",
    "\n",
    "def load_data(mod_path, contri_path, coeff_path, var_map_path):\n",
    "    df_mod = pd.read_csv(mod_path)\n",
    "    df_contri = pd.read_csv(contri_path)\n",
    "    coef_data = pd.read_csv(coeff_path)\n",
    "    var_map = pd.read_csv(var_map_path)\n",
    "\n",
    "    return df_mod, df_contri, coef_data, var_map\n",
    "\n",
    "def preprocess_dates(df_mod, df_contri):\n",
    "\n",
    "    df_contri['Time Variable'] = pd.to_datetime(df_contri['Time Variable'], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "    df_contri.rename(columns={'Time Variable': 'Date'}, inplace=True)\n",
    "\n",
    "    def unify_date_format(df, date_column='Date'):\n",
    "        def to_int(date_str):\n",
    "            try:\n",
    "                return int(date_str)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "        \n",
    "        date_list = list(df[date_column].str.split(r'[\\/\\.\\-]').str[0].map(to_int))\n",
    "        max_date = max([date for date in date_list if date <= 31])\n",
    "        \n",
    "        if max_date <= 12:\n",
    "            dayfirst = False  # month first\n",
    "        else:\n",
    "            dayfirst = True   # day first\n",
    "        \n",
    "        df[date_column] = pd.to_datetime(df[date_column], format=\"mixed\", errors=\"coerce\", dayfirst=dayfirst).dt.strftime('%d-%m-%Y')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    df_mod = unify_date_format(df_mod, 'Date')\n",
    "    df_contri = unify_date_format(df_contri, 'Date')\n",
    "\n",
    "    return df_mod, df_contri\n",
    "\n",
    "def merge_and_process(df_mod, df_contri):\n",
    "    merged_df = pd.merge(df_contri, df_mod, on=['Mother_SKU', 'Date'], how='left', suffixes=('', '_mod'))\n",
    "\n",
    "    for col in df_contri.columns:\n",
    "        if col + '_mod' in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col + '_mod']\n",
    "            merged_df.drop(columns=[col + '_mod'], inplace=True)\n",
    "\n",
    "    merged_df = merged_df.fillna(0)\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "    merged_df['Year'] = merged_df['Date'].dt.year\n",
    "    merged_df['Month'] = merged_df['Date'].dt.strftime('%b')\n",
    "    merged_df['Quarter'] = merged_df['Date'].dt.quarter.apply(lambda x: f'Q{x}')\n",
    "\n",
    "    holiday_columns = [col for col in merged_df.columns if 'Holiday' in col]\n",
    "    melt_df = pd.melt(merged_df, id_vars=[col for col in merged_df.columns if col not in holiday_columns],\n",
    "                        value_vars=holiday_columns, var_name='Holiday_Type', value_name='Holiday')\n",
    "    melt_df['Holiday_Status'] = melt_df.apply(\n",
    "        lambda row: 'Non-Holiday' if row['Holiday'] == 0 else row['Holiday_Type'], axis=1)\n",
    "    melt_df.drop(columns=['Holiday_Type', 'Holiday'], inplace=True)\n",
    "    melt_df.rename(columns={'Holiday_Status': 'Holiday'}, inplace=True)\n",
    "    raw_df = melt_df.copy()\n",
    "\n",
    "    return raw_df\n",
    "\n",
    "def actual_dod(data, reg_price):\n",
    "\n",
    "    mosku = list(data.keys())[0]\n",
    "    actual_dod = {\n",
    "            'Mother_SKU' : mosku,\n",
    "            'Q1' : data[mosku]['Original_Quarter_Table'][2]['Q1']/100,\n",
    "            'Q2' : data[mosku]['Original_Quarter_Table'][2]['Q2']/100,\n",
    "            'Q3' : data[mosku]['Original_Quarter_Table'][2]['Q3']/100,\n",
    "            'Q4' : data[mosku]['Original_Quarter_Table'][2]['Q4']/100,\n",
    "            reg_price : data[mosku]['Original_Quarter_Table'][0]['Q1']\n",
    "    }\n",
    "\n",
    "    act_DOD = pd.DataFrame([actual_dod]) \n",
    "   \n",
    "\n",
    "    return act_DOD\n",
    "\n",
    "def input_dod(data, reg_price):\n",
    "\n",
    "    mosku = list(data.keys())[0]\n",
    "    input_dod = {\n",
    "            'Mother_SKU' : mosku,\n",
    "            'Q1' : data[mosku]['Quarter_Table'][2]['Q1']/100,\n",
    "            'Q2' : data[mosku]['Quarter_Table'][2]['Q2']/100,\n",
    "            'Q3' : data[mosku]['Quarter_Table'][2]['Q3']/100,\n",
    "            'Q4' : data[mosku]['Quarter_Table'][2]['Q4']/100,\n",
    "            reg_price : data[mosku]['Quarter_Table'][0]['Q1']\n",
    "    }\n",
    "\n",
    "    input_DOD = pd.DataFrame([input_dod]) \n",
    "   \n",
    "\n",
    "    return input_DOD\n",
    "\n",
    "def multi_data_input(data, var_map):\n",
    "\n",
    "    mosku = list(data.keys())[0]\n",
    "    promo_data = []\n",
    "    for promo in data[mosku]['Promo_Table']:\n",
    "        promo_dict = {\n",
    "            'Mother_SKU': mosku,\n",
    "            'PromotionType': promo['PromotionType'],\n",
    "            'Q1': promo['HistoricallyExisted'][0],\n",
    "            'Q2': promo['HistoricallyExisted'][1],\n",
    "            'Q3': promo['HistoricallyExisted'][2],\n",
    "            'Q4': promo['HistoricallyExisted'][3]\n",
    "        }\n",
    "        promo_data.append(promo_dict)\n",
    "\n",
    "    promo_df = pd.DataFrame(promo_data)\n",
    "    promo_melt = pd.melt(promo_df, id_vars=['Mother_SKU', 'PromotionType'], var_name='Quarter', value_name='Value')\n",
    "    promo_melt['Value'] = promo_melt['Value'].map({'Yes': 1, 'No': 0})\n",
    "    multi_data = promo_melt.pivot_table(index=['Mother_SKU', 'Quarter'], columns='PromotionType', values='Value', aggfunc='first')\n",
    "    multi_data.reset_index(inplace=True)\n",
    "    multi_data.columns.name = None\n",
    "    rename_dict = dict(zip(var_map['original'], var_map['replacement']))\n",
    "    multi_data = multi_data.rename(columns=rename_dict)\n",
    "\n",
    "    return multi_data\n",
    "\n",
    "\n",
    "def simulation(raw_df, coef_data, input_DOD, act_DOD, multi_data, reg_price, dod, unitcost):\n",
    "    # Round and log-transform input data\n",
    "    input_DOD = round(input_DOD, 2)\n",
    "    input_DOD1 = input_DOD.copy()\n",
    "    input_DOD[reg_price] = np.log(input_DOD[reg_price])\n",
    "    input_DOD.set_index(['Mother_SKU'], inplace=True)\n",
    "\n",
    "    act_DOD = round(act_DOD, 2)\n",
    "    act_DOD1 = act_DOD.copy()\n",
    "    act_DOD[reg_price] = np.log(act_DOD[reg_price])\n",
    "    act_DOD.set_index(['Mother_SKU'], inplace=True)\n",
    "\n",
    "    # Prepare raw data and coefficient data\n",
    "    Z = raw_df[['Mother_SKU', 'Date', 'Quarter']]\n",
    "    unit_cost = raw_df.groupby(['Mother_SKU'])[unitcost].mean().reset_index()\n",
    "    raw_df.set_index(['Mother_SKU', 'Date', 'Quarter'], inplace=True)\n",
    "    \n",
    "    coef_data.rename(columns={'Regular Price Variable': reg_price, 'DOD Variable': dod}, inplace=True)\n",
    "    merged_coef = Z.merge(coef_data, on=['Mother_SKU'], how='left')\n",
    "    merged_coef.set_index(['Mother_SKU', 'Date', 'Quarter'], inplace=True)\n",
    "    merged_coef = merged_coef.reindex(index=raw_df.index)\n",
    "\n",
    "    # Calculate historical predictions\n",
    "    Historical_pre = pd.DataFrame()\n",
    "    common_columns = merged_coef.columns.intersection(raw_df.columns)\n",
    "    for column_name in common_columns:\n",
    "        if pd.api.types.is_numeric_dtype(merged_coef[column_name]):\n",
    "            Historical_pre[column_name] = merged_coef[column_name] * raw_df[column_name]\n",
    "\n",
    "    # Simulate future values\n",
    "    dod_columns = Historical_pre.columns[Historical_pre.columns.str.endswith(\"_DOD\")]\n",
    "    dod_df = pd.DataFrame(1, index=multi_data.index, columns=dod_columns)\n",
    "    multi_data = pd.concat([multi_data, dod_df], axis=1)\n",
    "\n",
    "    Pre = pd.DataFrame()\n",
    "    Pre['XB'] = Historical_pre.sum(axis=1)\n",
    "    Pre['base'] = np.log(raw_df['Predicted']) - Pre['XB']\n",
    "    \n",
    "    multi_dod_reg = input_DOD / act_DOD\n",
    "    df = multi_dod_reg.reset_index()\n",
    "    melted_df = df.melt(id_vars=['Mother_SKU', reg_price], var_name='Quarter', value_name=dod)\n",
    "    melted_df = melted_df.sort_values(by=['Mother_SKU', 'Quarter'])\n",
    "    melted_df1 = melted_df[['Mother_SKU', 'Quarter', dod, reg_price]].reset_index(drop=True)\n",
    "\n",
    "    merged_multi1 = multi_data.merge(melted_df1, on=['Mother_SKU', 'Quarter'], how='left')\n",
    "    merged_multi = Z.merge(merged_multi1, on=['Mother_SKU', 'Quarter'], how='left')\n",
    "    merged_multi.set_index(['Mother_SKU', 'Date', 'Quarter'], inplace=True)\n",
    "\n",
    "    Simulated_con = Historical_pre * merged_multi\n",
    "    Simulated = pd.DataFrame()\n",
    "    Simulated_Vol = pd.DataFrame()\n",
    "    Simulated['Base'] = Pre['base']\n",
    "    Simulated['XB'] = Simulated_con.sum(axis=1)\n",
    "    Simulated_Vol['Volume'] = np.exp(Simulated['Base'] + Simulated['XB'])\n",
    "\n",
    "    Simulated_Vol = Simulated_Vol.reset_index()\n",
    "    Simulated_Vol = Simulated_Vol[['Mother_SKU', 'Quarter', 'Volume']]\n",
    "    Simulated_Vol = Simulated_Vol.groupby(['Mother_SKU', 'Quarter'])['Volume'].sum().reset_index()\n",
    "\n",
    "    # Calculate financial metrics\n",
    "    unit_cost.set_index(['Mother_SKU'], inplace=True)\n",
    "    unit_cost = unit_cost.reindex(index=act_DOD.index)\n",
    "\n",
    "    pre_vol = raw_df['Predicted'].reset_index()\n",
    "    pre_vol = pre_vol[['Mother_SKU', 'Quarter', 'Predicted']]\n",
    "    pre_vol = pre_vol.groupby(['Mother_SKU', 'Quarter'])['Predicted'].sum().reset_index()\n",
    "\n",
    "    melted_df1 = input_DOD1.melt(id_vars=[\"Mother_SKU\", reg_price], var_name=\"Quarter\", value_name=\"Fut_DOD\")\n",
    "    melted_df2 = act_DOD1.melt(id_vars=['Mother_SKU', reg_price], var_name='Quarter', value_name='His_DOD')\n",
    "\n",
    "    melted_df1 = melted_df1.sort_values(by=['Mother_SKU', 'Quarter']).reset_index(drop=True)\n",
    "    melted_df2 = melted_df2.sort_values(by=['Mother_SKU', 'Quarter']).reset_index(drop=True)\n",
    "\n",
    "    calculation = melted_df1.merge(melted_df2, on=['Mother_SKU', 'Quarter'], how='left')\n",
    "    calculation = calculation.merge(unit_cost, on=['Mother_SKU'], how='left')\n",
    "    calculation = calculation.merge(pre_vol, on=['Mother_SKU', 'Quarter'], how='left')\n",
    "    calculation = calculation.merge(Simulated_Vol, on=['Mother_SKU', 'Quarter'], how='left')\n",
    "\n",
    "    calculation['Avg_selling_price'] = calculation[f'{reg_price}_x'] * (1 - calculation['His_DOD'])\n",
    "    calculation['Fut_selling_price'] = calculation[f'{reg_price}_y'] * (1 - calculation['Fut_DOD'])\n",
    "    calculation['Base_Revenue'] = calculation['Predicted'] * calculation['Avg_selling_price']\n",
    "    calculation['Future_Revenue'] = calculation['Volume'] * calculation['Fut_selling_price']\n",
    "    calculation['Base_Margin'] = calculation['Predicted'] * (calculation['Avg_selling_price'] - calculation[unitcost])\n",
    "    calculation['Future_Margin'] = calculation['Volume'] * (calculation['Fut_selling_price'] - calculation[unitcost])\n",
    "    calculation['Base Investment'] = calculation['Predicted'] * (calculation[f'{reg_price}_x'] - calculation['Avg_selling_price'])\n",
    "    calculation['Future Investment'] = calculation['Volume'] * (calculation[f'{reg_price}_y'] - calculation['Fut_selling_price'])\n",
    "\n",
    "    Final_calculation = pd.DataFrame()\n",
    "    Final_calculation[['Mother_SKU', 'PREDICTED_Volume', 'Future_Volume', 'Base_Revenue', 'Future_Revenue', 'Base_Margin', 'Future_Margin', 'Base Investment', 'Future Investment']] = calculation[['Mother_SKU', 'Predicted', 'Volume', 'Base_Revenue', 'Future_Revenue', 'Base_Margin', 'Future_Margin', 'Base Investment', 'Future Investment']]\n",
    "\n",
    "    sim_result = Final_calculation.groupby(['Mother_SKU']).sum().reset_index()\n",
    "    sim_result = round(sim_result, 2)\n",
    "\n",
    "    # Scale future parameters keeping Base as 100%\n",
    "    sim_result['Future_Revenue'] = (sim_result['Future_Revenue'] / sim_result['Base_Revenue']) * 100\n",
    "    sim_result['Future_Margin'] = (sim_result['Future_Margin'] / sim_result['Base_Margin']) * 100\n",
    "    sim_result['Future_Volume'] = (sim_result['Future_Volume'] / sim_result['PREDICTED_Volume']) * 100\n",
    "    sim_result['Future Investment'] = (sim_result['Future Investment'] / sim_result['Base Investment']) * 100\n",
    "\n",
    "    # Original overall metrics calculation\n",
    "    overall_metrics = [\n",
    "        {\n",
    "            \"KPIs\": \"Revenue\",\n",
    "            \"Baseline\": 100,\n",
    "            \"Future_State_Scenario\": sim_result['Future_Revenue'].sum(),\n",
    "            \"Change\": round(sim_result['Future_Revenue'].sum() - 100)\n",
    "        },\n",
    "        {\n",
    "            \"KPIs\": \"Gross Margin\",\n",
    "            \"Baseline\": 100,\n",
    "            \"Future_State_Scenario\": sim_result['Future_Margin'].sum(),\n",
    "            \"Change\": round(sim_result['Future_Margin'].sum() - 100)\n",
    "        },\n",
    "        {\n",
    "            \"KPIs\": \"Volume\",\n",
    "            \"Baseline\": 100,\n",
    "            \"Future_State_Scenario\": sim_result['Future_Volume'].sum(),\n",
    "            \"Change\": round(sim_result['Future_Volume'].sum() - 100)\n",
    "        },\n",
    "        {\n",
    "            \"KPIs\": \"Investment\",\n",
    "            \"Baseline\": 100,\n",
    "            \"Future_State_Scenario\": sim_result['Future Investment'].sum(),\n",
    "            \"Change\": round(sim_result['Future Investment'].sum() - 100)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Collect individual quarter results\n",
    "    individual_quarters = calculation[['Mother_SKU', 'Quarter', 'Base_Revenue', 'Future_Revenue', 'Base_Margin', 'Future_Margin', 'Predicted', 'Volume', 'Base Investment', 'Future Investment']].copy()\n",
    "    individual_quarters.columns = ['Mother_SKU', 'Quarter', 'Base_Revenue', 'Future_Revenue', 'Base_Margin', 'Future_Margin', 'PREDICTED_Volume', 'Future_Volume', 'Base_Investment', 'Future_Investment']\n",
    "    individual_quarters = round(individual_quarters, 2)\n",
    "\n",
    "    # Prepare data for Graph1 and Graph2\n",
    "    graph1_data = {\n",
    "        \"Base_Revenue\": [],\n",
    "        \"Future_Revenue\": [],\n",
    "        \"Base_Volume\": [],\n",
    "        \"Future_Volume\": []\n",
    "    }\n",
    "    \n",
    "    graph2_data = {\n",
    "        \"Base_Margin\": [],\n",
    "        \"Future_Margin\": [],\n",
    "        \"Base_Investment\": [],\n",
    "        \"Future_Investment\": []\n",
    "    }\n",
    "\n",
    "    quarters = sorted(individual_quarters['Quarter'].unique())\n",
    "\n",
    "    for quarter in quarters:\n",
    "        base_revenue = individual_quarters[individual_quarters['Quarter'] == quarter]['Base_Revenue'].sum()\n",
    "        future_revenue = (individual_quarters[individual_quarters['Quarter'] == quarter]['Future_Revenue'].sum() / base_revenue) * 100\n",
    "        \n",
    "        base_volume = individual_quarters[individual_quarters['Quarter'] == quarter]['PREDICTED_Volume'].sum()\n",
    "        future_volume = (individual_quarters[individual_quarters['Quarter'] == quarter]['Future_Volume'].sum() / base_volume) * 100\n",
    "        \n",
    "        base_margin = individual_quarters[individual_quarters['Quarter'] == quarter]['Base_Margin'].sum()\n",
    "        future_margin = (individual_quarters[individual_quarters['Quarter'] == quarter]['Future_Margin'].sum() / base_margin) * 100\n",
    "        \n",
    "        base_investment = individual_quarters[individual_quarters['Quarter'] == quarter]['Base_Investment'].sum()\n",
    "        future_investment = (individual_quarters[individual_quarters['Quarter'] == quarter]['Future_Investment'].sum() / base_investment) * 100\n",
    "\n",
    "        graph1_data[\"Base_Revenue\"].append(100)\n",
    "        graph1_data[\"Future_Revenue\"].append(future_revenue)\n",
    "        graph1_data[\"Base_Volume\"].append(100)\n",
    "        graph1_data[\"Future_Volume\"].append(future_volume)\n",
    "\n",
    "        graph2_data[\"Base_Margin\"].append(100)\n",
    "        graph2_data[\"Future_Margin\"].append(future_margin)\n",
    "        graph2_data[\"Base_Investment\"].append(100)\n",
    "        graph2_data[\"Future_Investment\"].append(future_investment)\n",
    "\n",
    "    result = {\n",
    "        \"Overall_Metrics\": overall_metrics,\n",
    "        \"Graph1\": {\n",
    "            \"Base_Revenue\": graph1_data[\"Base_Revenue\"],\n",
    "            \"Future_Revenue\": graph1_data[\"Future_Revenue\"],\n",
    "            \"Base_Volume\": graph1_data[\"Base_Volume\"],\n",
    "            \"Future_Volume\": graph1_data[\"Future_Volume\"]\n",
    "        },\n",
    "        \"Graph2\": {\n",
    "            \"Base_Margin\": graph2_data[\"Base_Margin\"],\n",
    "            \"Future_Margin\": graph2_data[\"Future_Margin\"],\n",
    "            \"Base_Investment\": graph2_data[\"Base_Investment\"],\n",
    "            \"Future_Investment\": graph2_data[\"Future_Investment\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    json_result = json.dumps(result)\n",
    "    open(\"NEWDATA.json\", \"w\").write(json_result)\n",
    "\n",
    "    return json_result\n",
    "\n",
    "\n",
    "\n",
    "@simulationOutput.route(\"/simulationOutput\", methods=[\"POST\"])\n",
    "def main():\n",
    "    jdata = request.json\n",
    "    email = jdata.get(\"email\")\n",
    "    project_name = jdata.get(\"projectName\")\n",
    "    session_name = jdata.get(\"sessionName\")\n",
    "    data = jdata.get(\"simulationInput\")\n",
    "\n",
    "    user_model_run_res = model_run_result.find(\n",
    "        {\n",
    "            \"email\": email,\n",
    "            \"projectName\": project_name,\n",
    "            \"sessionName\": session_name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    recent_model = user_model_run_res.sort(\"modifiedAt\", DESCENDING).limit(1)\n",
    "\n",
    "    recent_model = list(recent_model)[0]\n",
    "\n",
    "    user_modelling_ads = get_user_modelling_ads(\n",
    "        email, project_name, session_name, recent_model[\"modellingBuild\"]\n",
    "    )\n",
    "\n",
    "    user_final_model = final_model_res_collection.find_one(\n",
    "        {\n",
    "            \"email\": email,\n",
    "            \"projectName\": project_name,\n",
    "            \"sessionName\": session_name,\n",
    "        }\n",
    "    )\n",
    "    if not user_final_model:\n",
    "        raise Exception(\"Final Model not found\")\n",
    "\n",
    "    file_path, file_name, col_map = file_details(email, project_name, session_name)\n",
    "\n",
    "    if col_map:\n",
    "\n",
    "        mod_path = user_modelling_ads[\"filePath\"][\"modelling_ads_path\"]\n",
    "        contri_path = user_final_model[\"finalFilePath\"][\"optimal_contributions_path\"]\n",
    "        coeff_path = user_final_model[\"finalFilePath\"][\"coefficients_path\"]\n",
    "        var_map_path = user_modelling_ads[\"filePath\"][\"variable_mapping_path\"]\n",
    "\n",
    "        reg_price = col_map[\"Regular Price Variable\"][0]\n",
    "        dod = col_map[\"DOD Variable\"][0]\n",
    "        unitcost = col_map[\"Unit Cost\"][0]\n",
    "\n",
    "        df_mod, df_contri, coef_data, var_map = load_data(mod_path, contri_path, coeff_path, var_map_path)\n",
    "        df_mod, df_contri = preprocess_dates(df_mod, df_contri)\n",
    "        raw_df = merge_and_process(df_mod, df_contri)\n",
    "        act_DOD = actual_dod(data, reg_price) # recieved from frontend \n",
    "        input_DOD = input_dod(data, reg_price) # recieved from frontend \n",
    "        multi_data = multi_data_input(data, var_map) # recieved from frontend \n",
    "        result = simulation(raw_df, coef_data, input_DOD, act_DOD, multi_data, reg_price, dod, unitcost)\n",
    "        \n",
    "        print(json.dumps(result, indent = 4)) \n",
    "\n",
    "        return result \n",
    "\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to fetch configuration from MongoDB\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
